{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-0gMvinTpag"
      },
      "source": [
        "# Calling and Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfKFj5t5Tpbh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "RFlOZouPTpbm",
        "outputId": "d6ce4edc-e677-4c74-9b05-f53961146784"
      },
      "outputs": [],
      "source": [
        "aapl = pd.read_csv('AAPL.csv')\n",
        "aapl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCPDzHR3Tpbr"
      },
      "outputs": [],
      "source": [
        "aapl = aapl[(aapl['Date']>= '2013-12-31') & (aapl['Date']<='2015-12-31')]\n",
        "aapl = aapl.copy()\n",
        "aapl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KBQC9mrTpbv"
      },
      "outputs": [],
      "source": [
        "aapl['Date'] = pd.to_datetime(aapl['Date'])\n",
        "aapl['Prev Adj'] = aapl['Adj Close'].shift(1)\n",
        "aapl['prev close'] = aapl['Close'].shift(1)\n",
        "aapl['Change'] = (aapl['Close'] - aapl['prev close'])/aapl['prev close']*100\n",
        "aapl['Sector'] = 'Consumer Goods'\n",
        "print(aapl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy78PtLTTpbz"
      },
      "outputs": [],
      "source": [
        "aapl['movement'] = aapl.apply(\n",
        "    lambda row: 1 if row['Adj Close'] > row['Prev Adj'] else 0,\n",
        "    axis = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeIunLcOTpb1"
      },
      "outputs": [],
      "source": [
        "aapl = aapl.iloc[1:].copy()\n",
        "aapl.drop(columns=['Prev Adj', 'prev close'], errors='ignore', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfwb5u0pTpb5"
      },
      "outputs": [],
      "source": [
        "aapl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmWejtnUTpcc"
      },
      "outputs": [],
      "source": [
        "data = aapl.copy()\n",
        "\n",
        "# 변수 정규화\n",
        "scaler = StandardScaler()\n",
        "data[['Open', 'High', 'Low', 'Volume', 'Adj Close','Close']] = scaler.fit_transform(data[['Open', 'High', 'Low', 'Volume', 'Adj Close','Close']])\n",
        "\n",
        "# 학습 세트: 2014년 1월 1일 ~ 2015년 8월 1일\n",
        "train_data = data[(data['Date'] >= '2014-01-01') & (data['Date'] < '2015-10-01')]\n",
        "\n",
        "# 개발 세트: 2015년 8월 1일 ~ 2015년 10월 1일\n",
        "# valid_data = data[(data['Date'] >= '2015-08-01') & (data['Date'] < '2015-10-01')]\n",
        "\n",
        "# 테스트 세트: 2015년 10월 1일 ~ 2016년 1월 1일\n",
        "test_data = data[(data['Date'] >= '2015-10-01') & (data['Date'] < '2016-01-01')]\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, data, seq_length):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "        self.features = data[['Open','High','Low','Volume','Adj Close','Close']].values\n",
        "        self.labels = data['movement'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length   #데이터 길이 반환\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx:idx+self.seq_length]  #idx부터 seq만큼의 피처 데이터\n",
        "        y = self.labels[idx+self.seq_length] #seq 다음 날의 레이블 데이터\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "\n",
        "seq_length = 5 #5일치로 하루 예측\n",
        "\n",
        "# train_data, test_data = train_test_split(data, test_size=0.2,shuffle=False)\n",
        "\n",
        "train_dataset = StockDataset(train_data, seq_length)\n",
        "test_dataset = StockDataset(test_data, seq_length)\n",
        "# valid_dataset = StockDataset(valid_data, seq_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "# valid_dataset = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4GiZCaDGiGA"
      },
      "outputs": [],
      "source": [
        "sentiment = pd.read_csv('allsenti_AAPL.csv')\n",
        "sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEOlRFuI2hkR"
      },
      "outputs": [],
      "source": [
        "sentiment['filename'] = pd.to_datetime(sentiment['filename'])\n",
        "filtered_sentiment = sentiment[sentiment['filename'].isin(aapl['Date'])]\n",
        "filtered_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGOlOGoxTpcn"
      },
      "outputs": [],
      "source": [
        "sentiment_data = filtered_sentiment.copy()\n",
        "\n",
        "data = aapl.copy()\n",
        "\n",
        "# 변수 정규화\n",
        "scaler = StandardScaler()\n",
        "data[['Open', 'High', 'Low', 'Volume', 'Adj Close','Close']] = scaler.fit_transform(data[['Open', 'High', 'Low', 'Volume', 'Adj Close','Close']])\n",
        "\n",
        "# 학습 세트: 2014년 1월 1일 ~ 2015년 8월 1일\n",
        "train_data = data[(data['Date'] >= '2014-01-01') & (data['Date'] < '2015-10-01')]\n",
        "train_sentiment = sentiment_data[(sentiment_data['filename'] >= '2014-01-01') & (sentiment_data['filename'] < '2015-10-01')]\n",
        "\n",
        "train_data.reset_index(drop=True, inplace=True)\n",
        "train_sentiment.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "# 테스트 세트: 2015년 10월 1일 ~ 2016년 1월 1일\n",
        "test_data = data[(data['Date'] >= '2015-10-01') & (data['Date'] < '2016-01-01')]\n",
        "test_sentiment = sentiment_data[(sentiment_data['filename'] >= '2015-10-01') & (sentiment_data['filename'] < '2016-01-01')]\n",
        "\n",
        "test_data.reset_index(drop=True, inplace=True)\n",
        "test_sentiment.reset_index(drop=True, inplace=True)\n",
        "\n",
        "class AddStockDataset(Dataset):\n",
        "    def __init__(self, data, seq_length, sentiment_data):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "        self.sentiment_data = sentiment_data\n",
        "        self.features = data[['Open','High','Low','Volume','Adj Close','Close']].values\n",
        "        self.labels = data['movement'].values\n",
        "        self.sentiment_data = sentiment_data[['positive','neutral','negative']].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length   #데이터 길이 반환\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx:idx+self.seq_length]  #idx부터 seq만큼의 피처 데이터\n",
        "        y = self.labels[idx+self.seq_length] #seq 다음 날의 레이블 데이터\n",
        "        sentiment = self.sentiment_data[idx+self.seq_length] #해당 인덱스의 감정 분석 결과\n",
        "\n",
        "        # print(x.shape)\n",
        "        # print(sentiment.shape)\n",
        "\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "        sentiment = torch.tensor(sentiment, dtype=torch.float32)\n",
        "\n",
        "        return x,y,sentiment\n",
        "\n",
        "\n",
        "seq_length = 5 #5일치로 하루 예측\n",
        "\n",
        "# train_data, test_data = train_test_split(data, test_size=0.2,shuffle=False)\n",
        "\n",
        "add_train_dataset = AddStockDataset(train_data, seq_length,train_sentiment)\n",
        "add_test_dataset = AddStockDataset(test_data, seq_length, test_sentiment)\n",
        "# valid_dataset = StockDataset(valid_data, seq_length, sentiment_data)\n",
        "\n",
        "add_train_loader = DataLoader(add_train_dataset, batch_size=32, shuffle=False)\n",
        "add_test_loader = DataLoader(add_test_dataset, batch_size=32, shuffle=False)\n",
        "# valid_dataset = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EJp1CVSUKMK"
      },
      "outputs": [],
      "source": [
        "# 학습 세트: 2014년 1월 1일 ~ 2015년 8월 1일 사이의 20,339개 변동.\n",
        "# 개발 세트: 2015년 8월 1일 ~ 2015년 10월 1일 사이의 2,555개 변동.\n",
        "# 테스트 세트: 2015년 10월 1일 ~ 2016년 1월 1일 사이의 3,720개 변동.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Spu3oe5Tpb8"
      },
      "source": [
        "### Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dMAmqbxTpcf"
      },
      "outputs": [],
      "source": [
        "class BaseRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(BaseRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_size, output_size), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "      # h0 초기화 (num_layers,batch_size,hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  #RNN 마지막 타임스텝 출력을 fc layer에 통과\n",
        "        # print(out)\n",
        "        return out\n",
        "\n",
        "num_layers = 1\n",
        "input_size = 6\n",
        "hidden_size = 16\n",
        "output_size = 2  # 1 또는 0\n",
        "\n",
        "model = BaseRNN(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# out은 각 확률에 대한 확률 (batch_size,output_size) tensor\n",
        "# output_size=2 > [p(class 0),p[class 1]]\n",
        "# 큰 양수 : 해당 클래스 매우 높음, 큰 음수 : 매우 낮음, 0: 확률 0.5\n",
        "# 입력 데이터 x 형태 (batch_size, seq_length,input_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmVPZ36CTpch",
        "outputId": "1ab76ee4-9d38-4453-e72f-47ee3fdfce7e"
      },
      "outputs": [],
      "source": [
        "# 내부적으로 소프트맥스 적용하여 로짓-> 확률 변환, 확률과 실제 레이블 간 차이 계산\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for features, labels in train_loader:\n",
        "        outputs = model(features)\n",
        "        # labels = labels.unsqueeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Training complete')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Y0iOKtnOgaz8",
        "outputId": "69cded94-d04a-406e-db6b-9d8f9841be61"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiEg1O77c_Tm"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(loader, model):\n",
        "    model.eval()  # 평가 모드로 전환\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    tp = tn = fp = fn = 0\n",
        "\n",
        "    # 평가 단계이므로 파라미터 업데이트X\n",
        "    with torch.no_grad():\n",
        "        for features, labels in loader:\n",
        "            outputs = model(features)\n",
        "            # out = 각 클래스(오를 확률, 내릴 확률)에 대한 logit\n",
        "            # max함수를 통해 로짓에서 큰 값을 가진 인덱스 반환\n",
        "            # 최대값의 인덱스 = 예측된 클래스 레이블\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # total += labels.size(0)\n",
        "            # correct += (predicted == labels).sum().item()\n",
        "\n",
        "            tp += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "            tn += ((predicted == 0) & (labels == 0)).sum().item()\n",
        "            fp += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "            fn += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "\n",
        "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "    macc = (tp*tn-fp*fn) / np.sqrt( (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
        "\n",
        "    print(f'ACC: {accuracy:.4f}, MACC: {macc:.4f}')\n",
        "    print(f'TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}')\n",
        "    # print(outputs)\n",
        "# evaluate_model(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfOH3lECSBpm",
        "outputId": "69b69789-3a28-4fbd-f5eb-238d3096a4e7"
      },
      "outputs": [],
      "source": [
        "train_acc = evaluate_model(train_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP_Rg4yThqm9",
        "outputId": "771c119e-605e-4e36-f24c-f16fddd8fc88"
      },
      "outputs": [],
      "source": [
        "test_acc = evaluate_model(test_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moNoMcK2Tpcl"
      },
      "source": [
        "### Add Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaU-ikQc70Dx"
      },
      "outputs": [],
      "source": [
        "class StockRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, sentiment_size, num_layers):\n",
        "        super(StockRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.sentiment_size = sentiment_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Sequential(nn.Linear((hidden_size + sentiment_size), output_size), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x, sentiment_data):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        last_hidden = hn[-1]  # 마지막 hidden state 사용\n",
        "\n",
        "        sentiment_data = sentiment_data.view(sentiment_data.size(0), -1)\n",
        "        out = torch.cat((last_hidden, sentiment_data), dim=1)\n",
        "\n",
        "        output = self.fc(out)\n",
        "        return output\n",
        "\n",
        "num_layers = 1\n",
        "input_size = 6\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "sentiment_size = 3\n",
        "\n",
        "model = StockRNN(input_size, hidden_size, output_size, sentiment_size, num_layers)\n",
        "\n",
        "# print(\"Linear layer weights:\", model.fc[0].weight.shape)\n",
        "# print(\"Linear layer bias:\", model.fc[0].bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pslr0z6lMrIN"
      },
      "outputs": [],
      "source": [
        "def evaluate_add_model(loader, model):\n",
        "    model.eval()  # 평가 모드로 전환\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    tp = tn = fp = fn = 0\n",
        "\n",
        "    # 평가 단계이므로 파라미터 업데이트X\n",
        "    with torch.no_grad():\n",
        "        for features, labels, sentiment_data in loader:\n",
        "            outputs = model(features, sentiment_data)\n",
        "            # out = 각 클래스(오를 확률, 내릴 확률)에 대한 logit\n",
        "            # max함수를 통해 로짓에서 큰 값을 가진 인덱스 반환\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # total += labels.size(0)\n",
        "            # correct += (predicted == labels).sum().item()\n",
        "\n",
        "            tp += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "            tn += ((predicted == 0) & (labels == 0)).sum().item()\n",
        "            fp += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "            fn += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "\n",
        "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "    macc = (tp*tn-fp*fn) / np.sqrt( (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
        "\n",
        "    print(f'ACC: {accuracy:.4f}, MACC: {macc:.4f}')\n",
        "    print(f'TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlcV-H6iJSyD",
        "outputId": "c127358d-c47b-4999-ddab-bfb241f50740"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for features, labels, sentiment_data in add_train_loader:\n",
        "\n",
        "        features = features.view(-1,5,input_size)\n",
        "\n",
        "        outputs = model(features, sentiment_data)\n",
        "\n",
        "        # labels = labels.unsqueeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Training complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm21_aljNH7s",
        "outputId": "96836d7a-309a-48ac-e1a6-f4f4edb8464b"
      },
      "outputs": [],
      "source": [
        "train_acc = evaluate_add_model(add_train_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaIhiaT4nOAS",
        "outputId": "b55bb86a-6b7a-4e5a-92ae-b9da68215d8f"
      },
      "outputs": [],
      "source": [
        "test_acc = evaluate_add_model(add_test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Ix9X-0nenTYQ",
        "outputId": "40659f92-aaeb-4a3c-d38c-daa5071f8349"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni-aC0yOjLkt"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnbucv5wTpb_"
      },
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    x = (x-x.min()) / (x.max()-x.min())\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpsv3VHGTpcB"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    d_open = normalize(data['Open'].values)\n",
        "    d_close = normalize(data['Close'].values)\n",
        "    d_high = normalize(data['High'].values)\n",
        "    d_low = normalize(data['Low'].values)\n",
        "    d_volume = normalize(data['Volume'].values)\n",
        "    d_adj = normalize(data['Adj Close'])\n",
        "    d_movement = data['movement']\n",
        "\n",
        "    x = np.stack([d_open,d_close,d_high,d_low,d_volume,d_adj])\n",
        "\n",
        "    return x,d_movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mu-oXNoTpcD"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size,hidden_dim,n_layers):\n",
        "        super(RNN,self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.RNN(input_size,hidden_dim,n_layers,batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "        # x (batch_size,seq_length,input_size)\n",
        "        # hidden (n_layers, batch_size, hidden_dim)\n",
        "        # r_out (batch_size, time_step, hidden_size)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        r_out, hidden = self.rnn(x, hidden)\n",
        "        r_out = r_out.view(-1, self.hidden_dim)\n",
        "\n",
        "        output = self.fc(r_out)\n",
        "\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olAoSWIPTpcG",
        "outputId": "0715d0f5-78a8-4b19-be3c-d1da9cde9b6c"
      },
      "outputs": [],
      "source": [
        "# dimension 확인\n",
        "seq_length = 20\n",
        "test_rnn = RNN(input_size=1, output_size=1, hidden_dim=10, n_layers=2)\n",
        "\n",
        "time_steps = np.linspace(0, np.pi, seq_length)\n",
        "data = np.sin(time_steps)\n",
        "data.resize((seq_length,1))\n",
        "\n",
        "test_input = torch.Tensor(data).unsqueeze(0)\n",
        "print('Input size:', test_input.size())\n",
        "\n",
        "test_output, test_h = test_rnn(test_input, None)\n",
        "print('output:', test_output.size())\n",
        "print('hidden:', test_h.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOkDJ8SDTpcQ",
        "outputId": "be5e7c41-6ed0-4b21-9927-cf58482ed7eb"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "hidden_dim = 32\n",
        "n_layers = 1\n",
        "\n",
        "rnn = RNN(input_size, output_size, hidden_dim, n_layers)\n",
        "print(rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUXDYgd_TpcW"
      },
      "outputs": [],
      "source": [
        "# train the RNN\n",
        "def train(rnn, n_steps, print_every):\n",
        "\n",
        "    # initialize the hidden state\n",
        "    hidden = None\n",
        "\n",
        "    for batch_i, step in enumerate(range(n_steps)):\n",
        "        # defining the training data\n",
        "        time_steps = np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1)\n",
        "        data = np.sin(time_steps)\n",
        "        data.resize((seq_length + 1, 1)) # input_size=1\n",
        "\n",
        "        x = data[:-1]\n",
        "        y = data[1:]\n",
        "\n",
        "        # convert data into Tensors\n",
        "        x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
        "        y_tensor = torch.Tensor(y)\n",
        "\n",
        "        # outputs from the rnn\n",
        "        prediction, hidden = rnn(x_tensor, hidden)\n",
        "\n",
        "        ## Representing Memory ##\n",
        "        # make a new variable for hidden and detach the hidden state from its history\n",
        "        # this way, we don't backpropagate through the entire history\n",
        "        hidden = hidden.data\n",
        "\n",
        "        # Loss and Optimization\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
        "        # calculate the loss\n",
        "        loss = criterion(prediction, y_tensor)\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        # perform backprop and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # display loss and predictions\n",
        "        if batch_i%print_every == 0:\n",
        "            print('Loss: ', loss.item())\n",
        "            plt.plot(time_steps[1:], x, 'r.') # input\n",
        "            plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.') # predictions\n",
        "            plt.show()\n",
        "\n",
        "    return rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9e4Zs6yPTpcZ",
        "outputId": "0ea0a97a-0798-4c64-e0f8-4febc2e7cc41"
      },
      "outputs": [],
      "source": [
        "# train the rnn and monitor results\n",
        "n_steps = 75\n",
        "print_every = 15\n",
        "\n",
        "trained_rnn = train(rnn, n_steps, print_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaK3G65u67n3"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl9MVzwBMSx5"
      },
      "outputs": [],
      "source": [
        "class BaseLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(BaseLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) #drop out, bidirectional=True\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_size, output_size), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))  #LSTM의 output은 Output과 마지막 타임스텝의 (hidden state, cell state)\n",
        "\n",
        "\n",
        "        output = self.fc(out[:,-1,:])\n",
        "        return output\n",
        "\n",
        "num_layers = 1\n",
        "input_size = 6\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "sentiment_size = 3\n",
        "\n",
        "model = BaseLSTM(input_size, hidden_size, output_size, num_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k69xhQtIRETF",
        "outputId": "ee06fd06-9bed-4784-8d56-a4b5dabf86d3"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for features, labels in train_loader:\n",
        "        outputs = model(features)\n",
        "        # labels = labels.unsqueeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Training complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rATWNFgsMuZu",
        "outputId": "db493742-7ae1-4502-981d-e8a118431eeb"
      },
      "outputs": [],
      "source": [
        "train_acc = evaluate_model(train_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M2sn2xtOUrR",
        "outputId": "99fa0fbd-87ec-40e2-ceb1-efeced0bb03a"
      },
      "outputs": [],
      "source": [
        "test_acc = evaluate_model(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em0az-ch6-Cq"
      },
      "outputs": [],
      "source": [
        "class StockLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, sentiment_size, num_layers):\n",
        "        super(StockLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.sentiment_size = sentiment_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) #drop out, bidirectional=True\n",
        "        self.fc = nn.Sequential(nn.Linear((hidden_size + sentiment_size), output_size), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x, sentiment_data):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))  #LSTM의 output은 Output과 마지막 타임스텝의 (hidden state, cell state)\n",
        "        last_hidden = hn[-1]  # 마지막 hidden state 사용\n",
        "        # print('hidden',last_hidden)\n",
        "        sentiment_data = sentiment_data.view(sentiment_data.size(0), -1)\n",
        "        # print('sentiment', sentiment_data)\n",
        "        out = torch.cat((last_hidden, sentiment_data), dim=1)\n",
        "\n",
        "        output = self.fc(out)\n",
        "        return output\n",
        "\n",
        "num_layers = 1\n",
        "input_size = 6\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "sentiment_size = 3\n",
        "\n",
        "model = StockLSTM(input_size, hidden_size, output_size, sentiment_size, num_layers)\n",
        "\n",
        "# sigmoid를 거친 hidden tensor도 0~1, raw sentiment score도 0~1이므로 추가적 scaling은 필요하지 않을 것으로 판단\n",
        "# LSTM의 hidden tensor는 (batch_size, hidden_size) 형태, sentiment score tensor는 (batch_size, 3)\n",
        "# concat을 수행하면 (batch_size, hidden_dim + sentiment_dim) 형태의 텐서"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WgcRCfc_TBm",
        "outputId": "8596d06f-1578-4efc-e0b5-4f0a1b020111"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for features, labels, sentiment_data in add_train_loader:\n",
        "\n",
        "        features = features.view(-1,5,input_size)\n",
        "\n",
        "        outputs = model(features, sentiment_data)\n",
        "\n",
        "        # labels = labels.unsqueeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Training complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJWwyUad_Urd",
        "outputId": "f0af20c2-59dd-49d9-fe8b-fd24350ccd5f"
      },
      "outputs": [],
      "source": [
        "train_acc = evaluate_add_model(add_train_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBmcMr64_ZDe",
        "outputId": "169c2f62-5bbd-45d1-e6b4-9a0706368fd7"
      },
      "outputs": [],
      "source": [
        "test_acc = evaluate_add_model(add_test_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jPQz7eSTQ4x"
      },
      "source": [
        "### Calculating Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kapaBdHwSi2X",
        "outputId": "c1db3e17-7151-4810-859a-4eba55d93c6b"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# sentiment 데이터와 AAPL 데이터 병합\n",
        "merged_data = pd.merge(aapl, filtered_sentiment, left_on='Date', right_on='filename')\n",
        "\n",
        "# 필요한 열만 선택\n",
        "correlation_data = merged_data[['positive', 'negative', 'neutral','sentiment_score', 'movement']]\n",
        "\n",
        "# 상관계수 계산\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "print(correlation_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ecaCNV5TKks"
      },
      "outputs": [],
      "source": [
        "# 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6yMHVuOko7W",
        "outputId": "7e67fbc7-6264-4faf-ba4a-d893e0f7dcfc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# GitHub 저장소의 ZIP 파일 URL\n",
        "url = 'https://github.com/yumoxu/stocknet-dataset/archive/refs/heads/master.zip'\n",
        "\n",
        "# ZIP 파일을 다운로드하여 메모리로 읽기\n",
        "response = requests.get(url)\n",
        "zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
        "\n",
        "# ZIP 파일 내의 'stocknet-dataset-master/price/raw/' 폴더 경로\n",
        "price_folder_path = 'stocknet-dataset-master/price/raw/'\n",
        "\n",
        "# 모든 CSV 파일을 읽고 결합\n",
        "data_frames = []\n",
        "for file_name in zip_file.namelist():\n",
        "    if file_name.startswith(price_folder_path) and file_name.endswith('.csv'):\n",
        "        # 주식 이름을 파일명에서 추출 (예: 'raw/prices/appl.csv' -> 'appl')\n",
        "        stock_name = os.path.splitext(os.path.basename(file_name))[0]\n",
        "\n",
        "        # CSV 파일을 읽어 데이터프레임으로 변환\n",
        "        with zip_file.open(file_name) as file:\n",
        "            df = pd.read_csv(file)\n",
        "            # 주식 이름 열 추가\n",
        "            df['Stock'] = stock_name\n",
        "            data_frames.append(df)\n",
        "\n",
        "# 모든 데이터프레임을 하나로 결합\n",
        "combined_df = pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# 결합된 데이터프레임 확인\n",
        "print(combined_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNMTCAITkp_r",
        "outputId": "4324509a-cd00-4e94-9c24-ef74b5197176"
      },
      "outputs": [],
      "source": [
        "print(combined_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2P8oY2SRk13H",
        "outputId": "dcccfed2-e2f2-4c43-da28-ef3f51f9daac"
      },
      "outputs": [],
      "source": [
        "combined_df = combined_df[(combined_df['Date']>= '2013-12-31') & (combined_df['Date']<='2015-12-31')]\n",
        "combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgVwnl-Sqx_2"
      },
      "source": [
        "# sentiment rnn + price rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUKkt7rfq0UC"
      },
      "outputs": [],
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "          super(SentimentRNN, self).__init__()\n",
        "          self.num_layers = num_layers\n",
        "          self.hidden_size = hidden_size\n",
        "          self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "          self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "    def forward(self, x):\n",
        "          h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "          out, hn = self.rnn(x,h0)  #out은 모든 time step hidden state(batch_size, seq_length, hidden_size), hn은 마지막 time step hidden state(num_layers, batch_size, hidden_size)\n",
        "          out = self.layer_norm(out)\n",
        "          return hn[-1]\n",
        "\n",
        "class StockSentimentRNN(nn.Module):\n",
        "    def __init__(self, stock_input_size, sentiment_input_size, hidden_size,output_size, num_layers):\n",
        "        super(StockSentimentRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.stock_rnn = nn.RNN(stock_input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.stock_layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.sentiment_rnn = SentimentRNN(sentiment_input_size, hidden_size,num_layers)\n",
        "        self.fc = nn.Sequential(nn.Linear((hidden_size * 2), output_size), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, data, sentiment_data):\n",
        "        h0_stock = torch.zeros(self.stock_rnn.num_layers, data.size(0), self.stock_rnn.hidden_size).to(data.device)\n",
        "        stock_out, stock_hn = self.stock_rnn(data, h0_stock)\n",
        "        stock_out = self.stock_layer_norm(stock_out)\n",
        "        stock_hidden = stock_hn[-1]  # 마지막 hidden state\n",
        "\n",
        "        # 감정 데이터 RNN 처리\n",
        "        # seq_length = data.size(1)\n",
        "        # sentiment_data = sentiment_data.unsqueeze(1).repeat(1, seq_length,1) #시퀀스 길이에 맞게 차원 추가\n",
        "        # sentiment_data = sentiment_data.view(-1, data.size(1), sentiment_data.size(-1))\n",
        "        seq_length = data.size(1)\n",
        "        sentiment_data = sentiment_data.unsqueeze(1).repeat(1, seq_length, 1)  # (batch_size, seq_length, sentiment_input_size)\n",
        "        sentiment_out = self.sentiment_rnn(sentiment_data) #(batch_size, hidden_size)\n",
        "\n",
        "        # 두 hidden state를 결합\n",
        "        # layernorm 거친 두 마지막 hidden state concat\n",
        "        combined_hidden = torch.cat((stock_hidden, sentiment_out), dim=1)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        output = self.fc(combined_hidden)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwhfZykswPWJ"
      },
      "outputs": [],
      "source": [
        "stock_input_size = 6       # 주가 데이터의 feature 수\n",
        "sentiment_input_size = 3   # 감정 데이터의 feature 수\n",
        "hidden_size = 16           # RNN의 hidden size\n",
        "output_size = 2            # 최종 출력 클래스 수\n",
        "num_layers = 1             # RNN의 layer 수\n",
        "\n",
        "model = StockSentimentRNN(stock_input_size, sentiment_input_size, hidden_size, output_size, num_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5sLpqa1yNRE",
        "outputId": "4c4c759b-54cc-4c59-9e42-9b230136e560"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for data, labels, sentiment_data in add_train_loader:\n",
        "        data = data.view(-1,seq_length,stock_input_size)\n",
        "        sentiment_data = sentiment_data  # (배치 크기, seq_length, sentiment_input_size)\n",
        "        # labels = labels.view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data, sentiment_data)\n",
        "        # labels = labels.unsqueeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(add_train_loader)\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}')\n",
        "\n",
        "print('Training complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isuFAHTwy7G-"
      },
      "outputs": [],
      "source": [
        "def evaluate_add_model(loader, model):\n",
        "    model.eval()  # 평가 모드로 전환\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    tp = tn = fp = fn = 0\n",
        "\n",
        "    # 평가 단계이므로 파라미터 업데이트X\n",
        "    with torch.no_grad():\n",
        "        for data, labels, sentiment_data in loader:\n",
        "            outputs = model(data, sentiment_data)\n",
        "            # out = 각 클래스(오를 확률, 내릴 확률)에 대한 logit\n",
        "            # max함수를 통해 로짓에서 큰 값을 가진 인덱스 반환\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # total += labels.size(0)\n",
        "            # correct += (predicted == labels).sum().item()\n",
        "\n",
        "            tp += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "            tn += ((predicted == 0) & (labels == 0)).sum().item()\n",
        "            fp += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "            fn += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "\n",
        "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "    macc = (tp*tn-fp*fn) / np.sqrt( (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
        "\n",
        "    print(f'ACC: {accuracy:.4f}, MACC: {macc:.4f}')\n",
        "    print(f'TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnuEGF1Q0CPr",
        "outputId": "800f3309-405e-4eaf-a1ed-b88e349ea2ae"
      },
      "outputs": [],
      "source": [
        "train_acc = evaluate_add_model(add_train_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHIkcfNtAJu_",
        "outputId": "7d3683fd-2d83-46bb-e806-9072c8a33e6f"
      },
      "outputs": [],
      "source": [
        "test_acc = evaluate_add_model(add_test_loader,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlrA_raNBqe6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2-0gMvinTpag",
        "Ni-aC0yOjLkt",
        "5jPQz7eSTQ4x"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pyto",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
